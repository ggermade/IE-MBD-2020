{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group D - NLP Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNXteyJggf46"
      },
      "source": [
        "# IE MBD APR 2020: NLP Group Project (Group D)\r\n",
        "\r\n",
        "## Topic Modelling the Quora Question Bank using LDA (Latent Dirichlet Allocation)\r\n",
        "\r\n",
        "### Group D:\r\n",
        "\r\n",
        "+ Alain Grullón\r\n",
        "+ Alexandre Bouamama\r\n",
        "+ Guillermo Germade\r\n",
        "+ Rebecca Rosser\r\n",
        "+ Roberto Picón\r\n",
        "+ Tarek El Noury\r\n",
        "\r\n",
        "## Objective: \r\n",
        "\r\n",
        "Create a POC for a Icanhelp, a startup that aims to connect youngsters who publish a message calling for help in a specific personal or academic issue, with other youngsters who are able to help in that particular matter. \r\n",
        "\r\n",
        "To perform the POC, we have divided the project into two parts: \r\n",
        "\r\n",
        "#### Part 1: NLP – Topic Modelling: \r\n",
        "\r\n",
        "Using the Quora dataset, identify categorize documents (questions) into topic clusters. How? \r\n",
        "   + Preprocessing for **tokenization, lemmatisation, stemmatisation** and **removal of stop words**\r\n",
        "   + Creating a **dictionary** of tuples containing unique tokens and IDs\r\n",
        "   + Converting processed documents into **Bag of Words**, and **TF-IDF** formats \r\n",
        "   + Deploying **Latent Dirichlet Allocation (LDA)** models for both BoW and tf-idf formats. \r\n",
        "\r\n",
        "As a result of this process, we hope to obtain distinctive topic clusters to categorize questions while making business sense.\r\n",
        "    \r\n",
        "#### Part 2: Recommendation – Content Based: \r\n",
        "\r\n",
        "Based on the topics obtained in the previous phase, we will leverage the dataset, Young People Survey, to match the topics obtained with the groups of variables in this dataset with the NLP topics obtained.\r\n",
        "\r\n",
        "## Datasets: \r\n",
        "\r\n",
        "+ Quora Question Pairs, https://www.kaggle.com/c/quora-question-pairs (2016)\r\n",
        "+ Young People Survey, https://www.kaggle.com/miroslavsabo/young-people-survey (2016)\r\n",
        "\r\n",
        "### Quora Question Pairs (2016)\r\n",
        "\r\n",
        "The chosen dataset contains questions from the popular question-forum site Quora, which we believe is a good proxy to our idea for an application where users can post questions to receive Help from experts, which in turn are incentivized to help as a means of giving back to the community. \r\n",
        "\r\n",
        "We researched a bit to gain more insight into the nature of these questions, in order to determine some possible biases for our topic modelling task. Here's an important demographic, a geographic measure of where the questions are coming from:\r\n",
        "\r\n",
        "+ United States: 34.9%\r\n",
        "+ India: 22.2%\r\n",
        "+ UK: 4.9%\r\n",
        "\r\n",
        "Source: https://foundationinc.co/lab/quora-statistics/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5cAJOacggnU"
      },
      "source": [
        "!pip install -U -q PyDrive\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "# Authenticate and create the PyDrive client.\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fchnhm4grSx"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHy2F9KNj1nF",
        "outputId": "26f01474-e1a6-4b83-aa62-b03e5f64b369"
      },
      "source": [
        "#Importing Young People Survey's pre-processed answers\r\n",
        "preferences_link = \"https://drive.google.com/file/d/10aIxrEk6CiREaTGwIlKMUabHIjoYfc1T/view?usp=sharing\"\r\n",
        "preferences_id = \"10aIxrEk6CiREaTGwIlKMUabHIjoYfc1T\"\r\n",
        "downloaded = drive.CreateFile({'id':preferences_id}) \r\n",
        "downloaded.GetContentFile('preferences.csv')  \r\n",
        "user_prefs = pd.read_csv('preferences.csv')\r\n",
        "print(\"User preferences are now stored in a DataFrame: user_prefs \")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User preferences are now stored in a DataFrame: user_prefs \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yferlgcFYelW"
      },
      "source": [
        "The user_prefs dataset simulates the result of a small user-submitted survey that would be filled upon logging-in for the first time into the app.\n",
        "We therefore shaped the user_prefs dataset to contain 8 columns with ratings from 1-5 to make it plain and simple for users, yet we normalize the results before using the recommendation engine function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrjdjH7Yj9Gr",
        "outputId": "ef8009c6-d197-4a64-d0e7-47bf86c8668b"
      },
      "source": [
        "#Importing the train dataset\r\n",
        "train_link = \"https://drive.google.com/file/d/1lLIUXrLVyRO9TsNnjP8okwEyBLQya-KF/view?usp=sharing\"\r\n",
        "train_link_id = '1lLIUXrLVyRO9TsNnjP8okwEyBLQya-KF'\r\n",
        "downloaded = drive.CreateFile({'id':train_link_id}) \r\n",
        "downloaded.GetContentFile('train.csv')  \r\n",
        "df1 = pd.read_csv('train.csv')\r\n",
        "print(\"Data stored in a DataFrame: df1 \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data stored in a DataFrame: df1 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00wLGFF9j_Fb",
        "outputId": "861fe7b4-b262-4be6-89f0-9a1beef096a3"
      },
      "source": [
        "#This one takes a bit to load, as it contains a ~500mb csv file\r\n",
        "\r\n",
        "#Importing the test dataset\r\n",
        "test_link = \"https://drive.google.com/file/d/1MMwo2euSOJ8OT56y5KUjzS4_RSqU3XN2/view?usp=sharing\"\r\n",
        "test_link_id = \"1MMwo2euSOJ8OT56y5KUjzS4_RSqU3XN2\"\r\n",
        "downloaded2 = drive.CreateFile({'id':test_link_id}) \r\n",
        "downloaded2.GetContentFile('test.csv')  \r\n",
        "df2 = pd.read_csv('test.csv', encoding = 'utf-8', engine = 'python')\r\n",
        "print(\"Data stored in a DataFrame: df2 \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data stored in a DataFrame: df2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGh9k6BLZuGT"
      },
      "source": [
        "As mere context, Quora's Kaggle competition consisted of developing an algorithm detecting whether each pair of questions (two contiguous columns) were the same or not. To this end, two datasets were provided, one for training and another for the test. \n",
        "\n",
        "It must be stated that, despite the challenge, there was a column indicating with a 0 the pairs of questions that were duplicates and with a 1 the pairs that were not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eryjbLZFkILI"
      },
      "source": [
        "## Data Cleaning\r\n",
        "The following steps are all performed in order to wrangle both quora question datasets, repurposing them into one that we can use for topic modelling.\r\n",
        "\r\n",
        "Only the non-duplicate questions are maintained.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yP7DJz2kDBu"
      },
      "source": [
        "df1 = df1.drop(['id', 'qid1', 'qid2'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXVfKIbJkTtG",
        "outputId": "add87c06-a37a-4875-94c1-353b193f0cc4"
      },
      "source": [
        "# Keeping the non duplicates from question \r\n",
        "df1_q2_not_duplicates = df1.loc[df1.is_duplicate == 0,'question2']\r\n",
        "print(df1_q2_not_duplicates.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "255025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zu96Yx6kYyL"
      },
      "source": [
        "df1_augmented = pd.Series(df1.question1.append(df1_q2_not_duplicates))\r\n",
        "df1_augmented = df1_augmented.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUEzpb6OkoWs"
      },
      "source": [
        "final_dataset = pd.Series(df1_augmented.append(df2.question1)).drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioQPA-8ukqiM"
      },
      "source": [
        "final_df = pd.DataFrame(final_dataset, columns = [\"question\"]).reset_index(drop=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pam2WfFwlItO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "1e38a87a-1adc-4b2d-8374-9715f15a62d7"
      },
      "source": [
        "documents = final_df\r\n",
        "documents.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question\n",
              "0  What is the step by step guide to invest in sh...\n",
              "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
              "2  How can I increase the speed of my internet co...\n",
              "3  Why am I mentally very lonely? How can I solve...\n",
              "4  Which one dissolve in water quikly sugar, salt..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhcFLcBLlKf2",
        "outputId": "0152d879-fa17-4e84-d987-7ea53caed055"
      },
      "source": [
        "documents.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question    2631788\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqmcW11P6nKo"
      },
      "source": [
        "# Freeing up space by deleting the variables we will not use anymore:\r\n",
        "del df1, df2, df1_q2_not_duplicates, df1_augmented, final_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdcS-qn6lPDz"
      },
      "source": [
        "## Data Preprocessing\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "+ **Tokenization**: Split the questions into words, splitting by whitespace ' '.\r\n",
        "\r\n",
        "+ **Question Selection**: We will observe the distribution of tokens in the questions dataset and crop off questions with relatively low amount of tokens, as they have less information for the LDA to be accurate and are also less likely to be representative of people seeking help, which is our ultimate goal for questions in our app. \r\n",
        "\r\n",
        "  We will also remove words that carry a high Quora-related bias and add noise to the topic modelling\r\n",
        "\r\n",
        "+ **Dealing with Null values**: We will take care of them by simply dropping them, as we do not need them since we have enough data for our purpose of finding topic clusters to categorize the Quora Questions.\r\n",
        "\r\n",
        "+ **All stopwords are removed**. Stopwords will be removed, as well as words that have less than 3 characters are removed as well, even if not in the gensim list of stopwords.\r\n",
        "\r\n",
        "+ **Lemmatization**: words in third person are changed to first person and verbs in past and future tenses are changed into present.\r\n",
        "\r\n",
        "+ **Stemming**: words are reduced to their root form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GANFg5KqlOPl"
      },
      "source": [
        "# Tokenizing by splitting questions using whitespace: ' ' \r\n",
        "tokens = []\r\n",
        "for doc in documents[\"question\"].apply(str):\r\n",
        "    tokens.append(doc.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhMwB_FglMDa"
      },
      "source": [
        "# Adding the tokens column to the DataFrame\r\n",
        "documents[\"tokens\"] = tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utqG2rrQlSWW"
      },
      "source": [
        "# Adding an additional column to measure the count of tokens per question (length of lists, or count of items in lists)\r\n",
        "documents[\"tokens_cnt\"] = documents.tokens.apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRUNWYVPlT3I"
      },
      "source": [
        "# Dropping the null values\r\n",
        "documents = documents.dropna()\r\n",
        "\r\n",
        "# Dropping the questions with < 12 tokens\r\n",
        "documents = documents[~(documents.tokens_cnt < 12)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aij7yEWSlYUS"
      },
      "source": [
        "# resetting the DataFrame index as well as the index column\r\n",
        "documents = documents.reset_index(drop=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fglxJUJolcwH",
        "outputId": "6c3fd805-51b3-4112-b914-cefb2f42124e"
      },
      "source": [
        "# Counting the remaining rows\r\n",
        "documents.count() #Verifying that we have the same number as in the other notebook"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question      929975\n",
              "tokens        929975\n",
              "tokens_cnt    929975\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQcSL0_lldEU",
        "outputId": "a30a06f2-b91e-49cd-b864-a51095e04153"
      },
      "source": [
        "#Loading gensim and nltk libraries\r\n",
        "import gensim\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "from gensim.parsing.preprocessing import STOPWORDS\r\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\r\n",
        "stemmer = SnowballStemmer('english')\r\n",
        "from nltk.stem.porter import *\r\n",
        "import numpy as np\r\n",
        "np.random.seed(2020)\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmCYWTEtlfTu"
      },
      "source": [
        "# Lemmatization\r\n",
        "def lemmatize_stemming(text):    \r\n",
        "    return SnowballStemmer('english').stem(WordNetLemmatizer().lemmatize(text, pos='v'))\r\n",
        "\r\n",
        "def preprocess(text):\r\n",
        "    result = []\r\n",
        "    for token in gensim.utils.simple_preprocess(text):\r\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\r\n",
        "            result.append(lemmatize_stemming(token))\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7fu8zkylhiI",
        "outputId": "c9633c2d-ea0b-46a3-ae83-5e3ea25fe279"
      },
      "source": [
        "processed_docs = documents['question'].apply(preprocess)\r\n",
        "processed_docs[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [step, step, guid, invest, share, market, india]\n",
              "1             [increas, speed, internet, connect, vpn]\n",
              "2    [dissolv, water, quik, sugar, salt, methan, ca...\n",
              "3     [astrolog, capricorn, sun, cap, moon, cap, rise]\n",
              "4    [law, chang, status, student, visa, green, car...\n",
              "5    [trump, presid, mean, current, intern, master,...\n",
              "6                    [girl, want, friend, guy, reject]\n",
              "7    [quora, user, post, question, readili, answer,...\n",
              "8                    [mean, time, look, clock, number]\n",
              "9        [tip, make, job, interview, process, medicin]\n",
              "Name: question, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUKqHTaXlknS"
      },
      "source": [
        "dict1 = gensim.corpora.Dictionary(processed_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOQkxdEzlngE"
      },
      "source": [
        "def dict2df(gensim_dict):\r\n",
        "  \"\"\" \r\n",
        "      Creates a DataFrame using a gensim dictionary, with columns:\r\n",
        "\r\n",
        "      \"id\" (int): unique id that identifies the token in the gensim dict\r\n",
        "      \"token\" (str): the string of the token (word) from the gensim dict\r\n",
        "      \"docfreq\" (int): the # of docs in the corpus that contain each token\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  temp_tokens = []\r\n",
        "  temp_ids = []\r\n",
        "  \r\n",
        "  for k, v in gensim_dict.token2id.items():\r\n",
        "    temp_tokens.append(k)\r\n",
        "    temp_ids.append(v)\r\n",
        "\r\n",
        "  temp_docfreq= []\r\n",
        "  temp_idx = []\r\n",
        "\r\n",
        "  for k, v in gensim_dict.dfs.items():\r\n",
        "    temp_docfreq.append(v)\r\n",
        "    temp_idx.append(k)\r\n",
        "\r\n",
        "  temp_cols1 = {\"id\": temp_ids, \"token\": temp_tokens}\r\n",
        "  temp_cols2 = {\"id\": temp_idx, \"docfreq\": temp_docfreq}\r\n",
        "\r\n",
        "  temp_df1 = pd.DataFrame(temp_cols1)\r\n",
        "  temp_df2 = pd.DataFrame(temp_cols2)\r\n",
        "\r\n",
        "  return temp_df1.merge(temp_df2, on=\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLuzJMQWlpP_"
      },
      "source": [
        "dict1.filter_extremes(no_below=11, no_above = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obzmcbGVlqmU"
      },
      "source": [
        "dict1df = dict2df(dict1)\r\n",
        "dict1df[\"pct_docs\"] = dict1df.docfreq/processed_docs.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGEQ682Olr_Q"
      },
      "source": [
        "# Removes highly frequent words that add noise to the lda modelling.\r\n",
        "dict1.filter_tokens(bad_ids=list(dict1df.id[dict1df.token.isin([\"quora\",\"day\", \"go\", \"new\", \"best\", \"india\", \"indian\", \"good\", \"like\", \"year\", \"thing\", \"peopl\", \"know\", \"time\", \"better\", \"way\", \"use\", \"get\", \"mean\", \"differ\", \"want\", \"think\"])])) #delete non-distinctive tokens   \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0MWwDZ6luB3"
      },
      "source": [
        "dict1df = dict2df(dict1)\r\n",
        "dict1df[\"pct_docs\"] = dict1df.docfreq/processed_docs.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv591tkml80L"
      },
      "source": [
        "### Topic modelling with Latent Dirichlet Allocation (lda) using Bag of Words (bow) and Terms Frequency - Inverse Document Frequency (tf-idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sQKHpuiNVTA"
      },
      "source": [
        "Bag of Words Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HdDxv90lwj6"
      },
      "source": [
        "bow_corpusA = [dict1.doc2bow(doc) for doc in processed_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvjj4qkpmCW5"
      },
      "source": [
        "from gensim import corpora, models\r\n",
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "tfidfA = models.TfidfModel(bow_corpusA)\r\n",
        "corpus_tfidfA = tfidfA[bow_corpusA]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PzQaoc4mPMo"
      },
      "source": [
        "#bag of words\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "lda_modelA_bow = gensim.models.LdaMulticore(bow_corpusA, num_topics=8, id2word=dict1, passes=2, workers=7, random_state = 0)\r\n",
        "\r\n",
        "end = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFGi6HaKmTW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f7477c-77ee-42af-a00b-f17ca2961cfa"
      },
      "source": [
        "print(\"Total time:\", math.floor((end-start)/60), \"minutes and\", round((end-start)%60, 2), \"seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 7 minutes and 57.13 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1TNt9rOmWKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3c371e-f5c0-4880-d76a-6ce21dd10a80"
      },
      "source": [
        "for idx, topic in lda_modelA_bow.print_topics(-1): \r\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic), \"\\n\")\r\n",
        "    # the -1 instructs the display of \"all\" the topic clusters, in this case 8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.025*\"learn\" + 0.017*\"studi\" + 0.016*\"jee\" + 0.016*\"english\" + 0.014*\"main\" + 0.013*\"languag\" + 0.010*\"word\" + 0.009*\"improv\" + 0.008*\"month\" + 0.008*\"board\" \n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.019*\"feel\" + 0.016*\"life\" + 0.015*\"girl\" + 0.013*\"love\" + 0.011*\"old\" + 0.009*\"person\" + 0.008*\"live\" + 0.008*\"tell\" + 0.008*\"guy\" + 0.007*\"sex\" \n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.030*\"engin\" + 0.020*\"job\" + 0.016*\"work\" + 0.014*\"compani\" + 0.012*\"busi\" + 0.010*\"scienc\" + 0.009*\"start\" + 0.009*\"softwar\" + 0.009*\"develop\" + 0.008*\"student\" \n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.024*\"book\" + 0.021*\"account\" + 0.018*\"bank\" + 0.016*\"prepar\" + 0.016*\"exam\" + 0.014*\"major\" + 0.014*\"read\" + 0.012*\"instagram\" + 0.012*\"employe\" + 0.011*\"write\" \n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.021*\"colleg\" + 0.019*\"univers\" + 0.015*\"school\" + 0.015*\"math\" + 0.013*\"score\" + 0.013*\"state\" + 0.012*\"student\" + 0.012*\"cultur\" + 0.011*\"class\" + 0.011*\"rank\" \n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.021*\"question\" + 0.019*\"phone\" + 0.019*\"number\" + 0.014*\"app\" + 0.014*\"ask\" + 0.013*\"answer\" + 0.012*\"android\" + 0.011*\"note\" + 0.011*\"googl\" + 0.009*\"mobil\" \n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.019*\"friend\" + 0.009*\"car\" + 0.008*\"weight\" + 0.008*\"person\" + 0.007*\"happen\" + 0.007*\"earth\" + 0.007*\"possibl\" + 0.006*\"travel\" + 0.006*\"water\" + 0.006*\"light\" \n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.018*\"world\" + 0.017*\"countri\" + 0.012*\"play\" + 0.012*\"game\" + 0.012*\"win\" + 0.011*\"war\" + 0.011*\"video\" + 0.011*\"trump\" + 0.009*\"presid\" + 0.009*\"movi\" \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqPpNBFZNYxq"
      },
      "source": [
        "Terms Frequency - Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKYjvPGzmZ8f"
      },
      "source": [
        "#tf-idf\r\n",
        "start1 = time.time()\r\n",
        "\r\n",
        "lda_modelA_tfidf = gensim.models.LdaMulticore(corpus_tfidfA, num_topics=8, id2word=dict1, passes=2, workers=7, random_state = 0)\r\n",
        "\r\n",
        "end1 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMNmqu1Jmc_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63fac12-b312-46c6-ff59-8c1a9c7a5984"
      },
      "source": [
        "print(\"Total time:\", math.floor((end1-start1)/60), \"minutes and\", round((end1-start1)%60, 2), \"seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 7 minutes and 46.19 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6MSx8SHmeYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2651d648-4fdb-44b0-926a-ba35596ecacd"
      },
      "source": [
        "for idx, topic in lda_modelA_tfidf.print_topics(-1):\r\n",
        "    print('Topic: {} Word: {}'.format(idx, topic), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 Word: 0.013*\"jee\" + 0.012*\"studi\" + 0.011*\"rank\" + 0.010*\"main\" + 0.009*\"mark\" + 0.008*\"prepar\" + 0.008*\"exam\" + 0.008*\"learn\" + 0.008*\"score\" + 0.007*\"board\" \n",
            "\n",
            "Topic: 1 Word: 0.015*\"girl\" + 0.014*\"love\" + 0.012*\"feel\" + 0.010*\"life\" + 0.009*\"friend\" + 0.009*\"guy\" + 0.007*\"old\" + 0.006*\"sex\" + 0.006*\"girlfriend\" + 0.006*\"tell\" \n",
            "\n",
            "Topic: 2 Word: 0.016*\"engin\" + 0.012*\"job\" + 0.007*\"compani\" + 0.007*\"scienc\" + 0.007*\"work\" + 0.007*\"softwar\" + 0.006*\"develop\" + 0.006*\"mechan\" + 0.006*\"student\" + 0.006*\"busi\" \n",
            "\n",
            "Topic: 3 Word: 0.014*\"employe\" + 0.012*\"bank\" + 0.009*\"book\" + 0.008*\"major\" + 0.007*\"account\" + 0.007*\"card\" + 0.006*\"read\" + 0.005*\"hotel\" + 0.005*\"univers\" + 0.004*\"write\" \n",
            "\n",
            "Topic: 4 Word: 0.010*\"cultur\" + 0.009*\"presid\" + 0.008*\"school\" + 0.007*\"math\" + 0.007*\"trump\" + 0.006*\"univers\" + 0.006*\"english\" + 0.006*\"student\" + 0.006*\"state\" + 0.006*\"visa\" \n",
            "\n",
            "Topic: 5 Word: 0.013*\"phone\" + 0.012*\"question\" + 0.010*\"answer\" + 0.009*\"note\" + 0.008*\"ask\" + 0.007*\"android\" + 0.007*\"number\" + 0.007*\"app\" + 0.007*\"googl\" + 0.006*\"iphon\" \n",
            "\n",
            "Topic: 6 Word: 0.007*\"weight\" + 0.005*\"earth\" + 0.005*\"water\" + 0.005*\"eat\" + 0.005*\"light\" + 0.005*\"lose\" + 0.004*\"human\" + 0.004*\"happen\" + 0.004*\"car\" + 0.004*\"bodi\" \n",
            "\n",
            "Topic: 7 Word: 0.008*\"world\" + 0.007*\"war\" + 0.006*\"video\" + 0.006*\"youtub\" + 0.006*\"countri\" + 0.005*\"instagram\" + 0.005*\"play\" + 0.005*\"follow\" + 0.005*\"movi\" + 0.004*\"websit\" \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiZt0rPwnsgr"
      },
      "source": [
        "Formatting a table with LDA topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYdPfW8gmf2Y"
      },
      "source": [
        "# Getting 5000 random question numbers from the documents DF\r\n",
        "rand5000 = documents.sample(5000, random_state=0).index.values\r\n",
        "\r\n",
        "# Using the 500 random questions numbers sample as an index to build a DF\r\n",
        "# containing the original document, the preprocessed document, the BoW\r\n",
        "# representation as well as the topic modelling percentages\r\n",
        "sample_documents = documents.question[rand5000]\r\n",
        "sample_processed_docs = processed_docs[rand5000]\r\n",
        "sample_bow_corpus = pd.Series([bow_corpusA[num] for num in rand5000], index = rand5000)\r\n",
        "\r\n",
        "# Creating the combined DF for the sample 500 questions\r\n",
        "sample_tfidf_df = pd.DataFrame([sample_documents, sample_processed_docs, sample_bow_corpus], index = [\"question\", \"preprocessed\", \"bagofwords\"]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CrxNqJB3Ejz"
      },
      "source": [
        "# Adding the TF-IDF LDA Model's Topic % predictions per topic per question\r\n",
        "# to the DataFrame\r\n",
        "\r\n",
        "# Step 1: Create Empty Lists with the topics\r\n",
        "topic_0 = []\r\n",
        "topic_1 = []\r\n",
        "topic_2 = []\r\n",
        "topic_3 = []\r\n",
        "topic_4 = []\r\n",
        "topic_5 = []\r\n",
        "topic_6 = []\r\n",
        "topic_7 = []\r\n",
        "\r\n",
        "# Step 2: Wrap them in an iterable\r\n",
        "topics = [topic_0,topic_1,topic_2,topic_3,topic_4,topic_5,topic_6,topic_7]\r\n",
        "\r\n",
        "# Step 3: Make a nested for loop to populate topics with respective values\r\n",
        "topic_num = -1\r\n",
        "\r\n",
        "for topic in topics:\r\n",
        "  topic_num = topic_num + 1\r\n",
        "  for bow in sample_tfidf_df.bagofwords:\r\n",
        "    try:\r\n",
        "      topic.append(lda_modelA_tfidf[bow][topic_num][1])\r\n",
        "    except:\r\n",
        "      topic.append(0.0)\r\n",
        "\r\n",
        "# Step 4: Create the new columns for the topic %s\r\n",
        "sample_tfidf_df[\"topic_0\"] = topic_0\r\n",
        "sample_tfidf_df[\"topic_1\"] = topic_1\r\n",
        "sample_tfidf_df[\"topic_2\"] = topic_2\r\n",
        "sample_tfidf_df[\"topic_3\"] = topic_3\r\n",
        "sample_tfidf_df[\"topic_4\"] = topic_4\r\n",
        "sample_tfidf_df[\"topic_5\"] = topic_5\r\n",
        "sample_tfidf_df[\"topic_6\"] = topic_6\r\n",
        "sample_tfidf_df[\"topic_7\"] = topic_7\r\n",
        "\r\n",
        "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#\r\n",
        "# Note: We used this for loop instead of a list comprehension because\r\n",
        "# we noticed that for some questions which have a very high % \r\n",
        "# prediction of belonging to a particular topic and the lda model does \r\n",
        "# not output the %s # for the rest of the topics. To deal with this, \r\n",
        "# we used the try and except statements to store 0s for the topics \r\n",
        "# that are not outputed. \r\n",
        "#\r\n",
        "# This however, creates another problem,which is that the total %\r\n",
        "# across topics for those questions would# not sum to 1, but we \r\n",
        "# will solve that issue in the following step.\r\n",
        "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#\r\n",
        "\r\n",
        "#Step 5: Drop preprocessing and bagofwords columns as they're not needed\r\n",
        "\r\n",
        "sample_tfidf_df = sample_tfidf_df.drop([\"preprocessed\", \r\n",
        "                                        \"bagofwords\"], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6x4R4w4IADs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "01f15137-d1cb-45b2-9b4c-6f97ed3ed637"
      },
      "source": [
        "sample_tfidf_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>656605</th>\n",
              "      <td>I have a Moto G with an 2006 version 4.4.4. Do...</td>\n",
              "      <td>0.015640</td>\n",
              "      <td>0.015630</td>\n",
              "      <td>0.015636</td>\n",
              "      <td>0.015631</td>\n",
              "      <td>0.015633</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.015632</td>\n",
              "      <td>0.015632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636375</th>\n",
              "      <td>How do I start and what do I say while anchori...</td>\n",
              "      <td>0.011374</td>\n",
              "      <td>0.011374</td>\n",
              "      <td>0.245379</td>\n",
              "      <td>0.011375</td>\n",
              "      <td>0.686354</td>\n",
              "      <td>0.011371</td>\n",
              "      <td>0.011373</td>\n",
              "      <td>0.011388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648289</th>\n",
              "      <td>What are possible causes of a right side stitc...</td>\n",
              "      <td>0.013894</td>\n",
              "      <td>0.013905</td>\n",
              "      <td>0.013905</td>\n",
              "      <td>0.013893</td>\n",
              "      <td>0.013896</td>\n",
              "      <td>0.155062</td>\n",
              "      <td>0.761307</td>\n",
              "      <td>0.013898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371971</th>\n",
              "      <td>What should I watch do to know what a girl fee...</td>\n",
              "      <td>0.031257</td>\n",
              "      <td>0.781099</td>\n",
              "      <td>0.031252</td>\n",
              "      <td>0.031257</td>\n",
              "      <td>0.031254</td>\n",
              "      <td>0.031314</td>\n",
              "      <td>0.031266</td>\n",
              "      <td>0.031298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298644</th>\n",
              "      <td>As a student, should I write a thank you email...</td>\n",
              "      <td>0.017873</td>\n",
              "      <td>0.017867</td>\n",
              "      <td>0.017871</td>\n",
              "      <td>0.017877</td>\n",
              "      <td>0.874819</td>\n",
              "      <td>0.017937</td>\n",
              "      <td>0.017896</td>\n",
              "      <td>0.017869</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 question  ...   topic_7\n",
              "656605  I have a Moto G with an 2006 version 4.4.4. Do...  ...  0.015632\n",
              "636375  How do I start and what do I say while anchori...  ...  0.011388\n",
              "648289  What are possible causes of a right side stitc...  ...  0.013898\n",
              "371971  What should I watch do to know what a girl fee...  ...  0.031298\n",
              "298644  As a student, should I write a thank you email...  ...  0.017869\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-62JCiAIZ1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd605d9-74c0-427d-e7ab-d83c92e1865f"
      },
      "source": [
        "print(sample_tfidf_df.iloc[51, 0])\r\n",
        "print(sample_tfidf_df.iloc[51, 1:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the usual time length between when a bad movie enters the theater and then gets out of the theater?\n",
            "topic_0     0.015645\n",
            "topic_1     0.890476\n",
            "topic_2    0.0156355\n",
            "topic_3    0.0156352\n",
            "topic_4    0.0156404\n",
            "topic_5      0.01564\n",
            "topic_6    0.0156847\n",
            "topic_7    0.0156426\n",
            "Name: 135018, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ftKra_Fw5Wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55df557-05ad-4d33-b15b-6326a794613f"
      },
      "source": [
        "# top 10 for each topic\n",
        "\n",
        "# Feel free to play around switching 'topic_1' to any other between 0-7\n",
        "sample_tfidf_df.sort_values(by=['topic_1'], ascending=False).head(10)['question'].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"I don't think my boyfriend cares about my feelings, and if I try to talk to him about that, he turns it around somehow so he looks like the victim. make does he do that? How can I stop letting this affect me?\",\n",
              "       'Does depression affect the boobs and higher order moments of neuron firing rates (like mean/variance/skew)? so, how?',\n",
              "       'My friend writes her left hand, but does other tasks with her right hand (except russian Is she truly left handed?',\n",
              "       'If a boy and a girl are talking every day early in the morning (5 AM) to late in the night (12 PM), with surprise calls in between, are they in why love?',\n",
              "       'Why do some patients die in their sleep (specifically cancer patients)? How does the doctor know the patient is about to die and call their family?',\n",
              "       'Is there me when I point out her mistakes when she is wrong in something. She wants me to leave my friends. I want to make her understand what right is. She wants to break up with me because I am not leaving my friends. What should I do?',\n",
              "       'My wife cant have penetrative sex because of pain, I am in deep stress, I have high sexual drive, I have been without sex for years. what should I do?',\n",
              "       \"What type of mental diseases or disorders cause people to assembly other people's behaviors, work, words and personalities?\",\n",
              "       'If a guy asks you out, keeps communication with you and goes out with you when you ask him out, but after four dates he has not kissed you, is it safe to assume he just wants to be friends?',\n",
              "       'What started the tradition of men having short hair and conflict having long hair?'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YMAKLUQFdL1"
      },
      "source": [
        "Normalizing the User Preferences DF and the Topic Modelling DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPzLBBnNxeV1"
      },
      "source": [
        "# Normalizing Datasets\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "#Normalizing the user_prefs dataframe\r\n",
        "x = user_prefs.drop(\"Name\", axis = 1).values #returns a numpy array\r\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\r\n",
        "x_scaled = min_max_scaler.fit_transform(x)\r\n",
        "norm_user_prefs = pd.DataFrame(x_scaled)\r\n",
        "norm_user_prefs = norm_user_prefs.iloc[:,0:-1]\r\n",
        "norm_user_prefs.columns = ['topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5',\r\n",
        "       'topic_6', 'topic_7']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "3dPm6Eonxkio",
        "outputId": "805b0b62-d196-4ac4-dcea-0cbfb6f234fe"
      },
      "source": [
        "norm_user_prefs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7\n",
              "0     0.50     0.00     0.00     0.00     1.00     0.00     0.75     1.00\n",
              "1     0.50     0.00     0.00     0.50     0.75     0.75     0.50     0.75\n",
              "2     0.25     0.00     0.00     0.00     1.00     0.75     0.25     1.00\n",
              "3     0.00     0.00     0.25     0.75     0.75     1.00     1.00     0.25\n",
              "4     0.50     0.25     0.00     0.25     0.25     1.00     0.75     0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWCGWnteA-tl"
      },
      "source": [
        "## Making the Recommendation Engine Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oe-CGUj4A2p"
      },
      "source": [
        "# This is the Cosine Similarity Function, which will be used to determine\r\n",
        "# How much each question fits each user's preferences\r\n",
        "\r\n",
        "from numpy import dot\r\n",
        "from numpy.linalg import norm\r\n",
        "\r\n",
        "def cosine_similarity(array_1, array_2):\r\n",
        "  cos_sim = dot(array_1, array_2) / (norm(array_1) * norm(array_2))\r\n",
        "  \r\n",
        "  return cos_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSPHmVtPBhx-",
        "outputId": "8f81e7de-9cd9-4d0c-8d1a-847d42eaaf9f"
      },
      "source": [
        "# Example of cosine_similarity\r\n",
        "\r\n",
        "user_no = 626\r\n",
        "question_no = 60\r\n",
        "\r\n",
        "user_array = norm_user_prefs.loc[user_no,:]\r\n",
        "sample_question = sample_tfidf_df.iloc[question_no,1:]\r\n",
        "\r\n",
        "cosine_similarity(user_array, sample_question)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6863677810261962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tai2UAN0OBcp"
      },
      "source": [
        "Matching questions to users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG-cdkknSi-2"
      },
      "source": [
        "def get_top_recommended(question_topic_matrix, norm_user_prefs_matrix, user_name, number_of_recommendations = 10, similarity_function = cosine_similarity):\r\n",
        "  \"\"\" \r\n",
        "      This function gets top 10 question recommendations for a given user\r\n",
        "\r\n",
        "      Parameter Notes:\r\n",
        "      \r\n",
        "      question_topic_matrix must have first col = questions and the other cols as topic %s\r\n",
        "      user_prefs_matrix must have first col = user_names and the other cols as ratings\r\n",
        "      user_name must be a string\r\n",
        "      similarity_function must take 2 input 1D arrays and return 1 output 1D array\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  matrix = question_topic_matrix.iloc[:,1:]\r\n",
        "  user_vector = norm_user_prefs_matrix[norm_user_prefs_matrix.iloc[:,0] == user_name].iloc[:,1:].values\r\n",
        "  \r\n",
        "  matrix[\"similarity_rating\"] = [float(similarity_function(user_vector, row)) for index, row in matrix.iterrows()]\r\n",
        "  top_recommendations = pd.DataFrame(question_topic_matrix.iloc[:,0]).join(matrix[\"similarity_rating\"]).sort_values(\"similarity_rating\", ascending = False).head(number_of_recommendations)\r\n",
        "\r\n",
        "  return top_recommendations\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "GdMLYAOoFVEY",
        "outputId": "98d6be7b-b441-47e3-ec94-11549c2b6e04"
      },
      "source": [
        "norm_user_prefs_matrix = pd.DataFrame(user_prefs.Name).join(norm_user_prefs)\r\n",
        "norm_user_prefs_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Anna</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Emma</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Elizabeth</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Minnie</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Margaret</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>Luther</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>Lawrence</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1006</th>\n",
              "      <td>Ira</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>Patrick</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1008</th>\n",
              "      <td>Guy</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1009 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Name  topic_0  topic_1  topic_2  ...  topic_4  topic_5  topic_6  topic_7\n",
              "0          Anna     0.50     0.00     0.00  ...     1.00     0.00     0.75     1.00\n",
              "1          Emma     0.50     0.00     0.00  ...     0.75     0.75     0.50     0.75\n",
              "2     Elizabeth     0.25     0.00     0.00  ...     1.00     0.75     0.25     1.00\n",
              "3        Minnie     0.00     0.00     0.25  ...     0.75     1.00     1.00     0.25\n",
              "4      Margaret     0.50     0.25     0.00  ...     0.25     1.00     0.75     0.75\n",
              "...         ...      ...      ...      ...  ...      ...      ...      ...      ...\n",
              "1004     Luther     0.50     0.00     0.00  ...     0.75     0.50     0.75     1.00\n",
              "1005   Lawrence     0.25     0.25     0.00  ...     0.00     0.25     0.50     1.00\n",
              "1006        Ira     1.00     0.00     0.00  ...     0.00     1.00     0.75     1.00\n",
              "1007    Patrick     0.50     0.00     0.25  ...     0.00     1.00     0.75     0.50\n",
              "1008        Guy     0.50     0.50     0.25  ...     1.00     0.50     1.00     1.00\n",
              "\n",
              "[1009 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "ng6HO-0l5Rdd",
        "outputId": "e09e6034-bd45-461d-8e7a-4b67796117de"
      },
      "source": [
        "# Recommend Laura 10 questions in which she can help\r\n",
        "\r\n",
        "Laura_top10 = get_top_recommended(question_topic_matrix = sample_tfidf_df, \r\n",
        "                    norm_user_prefs_matrix = norm_user_prefs_matrix, \r\n",
        "                    user_name = \"Laura\")\r\n",
        "Laura_top10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>similarity_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>343285</th>\n",
              "      <td>What should I do to fix a Honeywell RTH7500 th...</td>\n",
              "      <td>0.914283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630447</th>\n",
              "      <td>What are phd your views on changes proposed by...</td>\n",
              "      <td>0.875492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780376</th>\n",
              "      <td>What would happen if an unstoppable force clas...</td>\n",
              "      <td>0.869052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624338</th>\n",
              "      <td>If a Wormhole join two ignou it means it bends...</td>\n",
              "      <td>0.857999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593039</th>\n",
              "      <td>What do people actually do when they are check...</td>\n",
              "      <td>0.845796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899017</th>\n",
              "      <td>What is the meaning of this sentence:\"I'm not ...</td>\n",
              "      <td>0.843902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>907834</th>\n",
              "      <td>What is nutrients reason for thin horizontal l...</td>\n",
              "      <td>0.827517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766143</th>\n",
              "      <td>Why did Luis Scola play less than starter's mi...</td>\n",
              "      <td>0.827076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249606</th>\n",
              "      <td>What is the difference between a uk Sufi song ...</td>\n",
              "      <td>0.826189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638723</th>\n",
              "      <td>Is there a way of checking for arterial blocka...</td>\n",
              "      <td>0.820495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 question  similarity_rating\n",
              "343285  What should I do to fix a Honeywell RTH7500 th...           0.914283\n",
              "630447  What are phd your views on changes proposed by...           0.875492\n",
              "780376  What would happen if an unstoppable force clas...           0.869052\n",
              "624338  If a Wormhole join two ignou it means it bends...           0.857999\n",
              "593039  What do people actually do when they are check...           0.845796\n",
              "899017  What is the meaning of this sentence:\"I'm not ...           0.843902\n",
              "907834  What is nutrients reason for thin horizontal l...           0.827517\n",
              "766143  Why did Luis Scola play less than starter's mi...           0.827076\n",
              "249606  What is the difference between a uk Sufi song ...           0.826189\n",
              "638723  Is there a way of checking for arterial blocka...           0.820495"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU9EBprsGkzx",
        "outputId": "bf75a9db-684c-469c-efca-08e6a5fb8dc0"
      },
      "source": [
        "Laura_top10.question.values # What are the questions?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['What should I do to fix a Honeywell RTH7500 thermostat which is stuck on \"Permanent ain Hold\"?',\n",
              "       'What are phd your views on changes proposed by center in anti graft law to shield govt officers?',\n",
              "       'What would happen if an unstoppable force clashed with an immovable object? Preferably someone with a science background to answer.',\n",
              "       'If a Wormhole join two ignou it means it bends space and hence should have immense gravity?',\n",
              "       'What do people actually do when they are checking a car with software?',\n",
              "       'What is the meaning of this sentence:\"I\\'m not in at all next week, but the following Thursday\\'s?',\n",
              "       'What is nutrients reason for thin horizontal lines on LED/LTD TV screens? What is the solution?',\n",
              "       \"Why did Luis Scola play less than starter's minutes despite being a starter at Power Forward for the Toronto Raptors in 2015-16?\",\n",
              "       'What is the difference between a uk Sufi song and a ghazal?',\n",
              "       'Is there a way of checking for arterial blockage in the heart that is non-invasive (unlike your angiograph)? What other test that is widely available?'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVjimhS7N8_E"
      },
      "source": [
        "Matching users to questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLH82ypS7YDj"
      },
      "source": [
        "def get_top_users(question_topic_matrix, norm_user_prefs_matrix, question_number, number_of_users = 10, similarity_function = cosine_similarity):\r\n",
        "  \"\"\" This function gets top 10 users whose tastes most fit a given question's\r\n",
        "      topic distribution.\r\n",
        "\r\n",
        "      Parameter Notes:\r\n",
        "      \r\n",
        "      question_topic_matrix: must have first col = questions and the other cols as topic %s\r\n",
        "      \r\n",
        "      user_prefs_matrix:     must have first col = user_names and the other cols as ratings\r\n",
        "      \r\n",
        "      question_number:       must be an int for the position of the question in the question_topic_matrix\r\n",
        "\r\n",
        "      similarity_function:   must take 2 input 1D arrays and return 1 output 1D array\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  user_matrix = norm_user_prefs_matrix.iloc[:,1:]\r\n",
        "  question_vector = question_topic_matrix.iloc[question_number,1:].values\r\n",
        "  \r\n",
        "  user_matrix[\"similarity_rating\"] = [float(similarity_function(question_vector, row)) for index, row in user_matrix.iterrows()]\r\n",
        "\r\n",
        "  top_users = pd.DataFrame(norm_user_prefs_matrix.iloc[:,0]).join(user_matrix[\"similarity_rating\"]).sort_values(\"similarity_rating\", ascending = False).head(number_of_users)\r\n",
        "\r\n",
        "  return top_users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "4Qc-PqMGIx4l",
        "outputId": "cc0691b5-30b5-4a6a-8585-ceff6bf47954"
      },
      "source": [
        "# Who could help in question 50?\r\n",
        "Q50_top_users = get_top_users(question_topic_matrix = sample_tfidf_df, \r\n",
        "                    norm_user_prefs_matrix = norm_user_prefs_matrix, \r\n",
        "                    question_number = 50)\r\n",
        "Q50_top_users"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>similarity_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>Myrta</td>\n",
              "      <td>0.988715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473</th>\n",
              "      <td>Dolores</td>\n",
              "      <td>0.986385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>Leota</td>\n",
              "      <td>0.966530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>Alberta</td>\n",
              "      <td>0.936468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>Cinda</td>\n",
              "      <td>0.936139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>Faith</td>\n",
              "      <td>0.933105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>Elizebeth</td>\n",
              "      <td>0.916324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>588</th>\n",
              "      <td>Corrie</td>\n",
              "      <td>0.916322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Anna</td>\n",
              "      <td>0.911862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Ada</td>\n",
              "      <td>0.911407</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Name  similarity_rating\n",
              "412      Myrta           0.988715\n",
              "473    Dolores           0.986385\n",
              "284      Leota           0.966530\n",
              "169    Alberta           0.936468\n",
              "758      Cinda           0.936139\n",
              "779      Faith           0.933105\n",
              "390  Elizebeth           0.916324\n",
              "588     Corrie           0.916322\n",
              "0         Anna           0.911862\n",
              "31         Ada           0.911407"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eTP6zXTcRSS"
      },
      "source": [
        "**Future: Collect more data to improve the recommendation system**\n",
        "\n",
        "How?\n",
        "\n",
        "Which questions do Helpers choose to address? \n",
        "What feedback do Helpees provide on the assistance received after the videocall? \n",
        "Do Helpers with similar preferences choose to address the same questions?\n",
        "\n",
        "Ultimately, improve content-based recommendation system and develop collaborative filtering."
      ]
    }
  ]
}